% Bab 3: Implementasi Lexer Sederhana (Hand-Written)
% File ini dapat dikompilasi terpisah atau sebagai bagian dari main.tex

\chapter{Implementasi Lexer Sederhana (Hand-Written)}
\label{chap:lexer-handwritten}

\section{Tujuan Pembelajaran}

Setelah mempelajari bab ini, mahasiswa diharapkan mampu:
\begin{enumerate}
    \item Memahami konsep dan struktur hand-written lexer
    \item Merancang state machine untuk token recognition
    \item Mengimplementasikan lexer sederhana dalam C/C++ untuk subset bahasa C
    \item Menangani whitespace, komentar (single-line dan multi-line), dan escape sequences
    \item Mengimplementasikan error handling untuk token tidak valid
    \item Membuat unit test untuk berbagai kasus input
\end{enumerate}

\section{Pendahuluan}

Setelah memahami konsep lexical analysis secara teori pada bab sebelumnya, pada bab ini kita akan mengimplementasikan lexer secara praktis menggunakan pendekatan \textbf{hand-written} (ditulis manual). Menurut sumber terbuka:

\begin{quote}
``Hand-written lexers are possible: directly code a state machine, or use manual scanning logic. Requires careful handling of edge cases (e.g. unclosed strings/comments).''\cite{opengenus2024lexer}
\end{quote}

Pendekatan hand-written memberikan kontrol penuh terhadap implementasi dan sangat berguna untuk pembelajaran karena mahasiswa dapat memahami setiap detail proses tokenization. Meskipun lebih kompleks dibanding menggunakan generator seperti Flex atau re2c, hand-written lexer memberikan fleksibilitas dan pemahaman yang lebih dalam.

\section{Struktur Token}

Sebelum mengimplementasikan lexer, kita perlu mendefinisikan struktur data untuk merepresentasikan token. Token minimal harus menyimpan:

\begin{enumerate}
    \item \textbf{Token Type}: Jenis token (identifier, keyword, number, operator, dll.)
    \item \textbf{Lexeme}: String aktual yang di-match dari source code
    \item \textbf{Position Information}: Baris dan kolom untuk error reporting
    \item \textbf{Value} (opsional): Nilai numerik untuk number literals
\end{enumerate}

\subsection{Token Types}

Token types dapat didefinisikan menggunakan enum. Berikut contoh untuk subset bahasa C:

\begin{lstlisting}[language=C++, caption=Definisi Token Types]
enum class TokenType {
    // Identifiers and Keywords
    IDENTIFIER,
    KEYWORD_INT, KEYWORD_FLOAT, KEYWORD_IF, KEYWORD_ELSE,
    KEYWORD_WHILE, KEYWORD_FOR, KEYWORD_RETURN,
    
    // Literals
    INTEGER_LITERAL,
    FLOAT_LITERAL,
    STRING_LITERAL,
    CHAR_LITERAL,
    
    // Operators
    OP_PLUS, OP_MINUS, OP_MULTIPLY, OP_DIVIDE,
    OP_ASSIGN, OP_EQUAL, OP_NOT_EQUAL,
    OP_LESS, OP_LESS_EQUAL, OP_GREATER, OP_GREATER_EQUAL,
    OP_AND, OP_OR, OP_NOT,
    
    // Punctuation
    SEMICOLON, COMMA, DOT,
    LPAREN, RPAREN,    // ( )
    LBRACE, RBRACE,    // { }
    LBRACKET, RBRACKET, // [ ]
    
    // Special
    END_OF_FILE,
    INVALID
};
\end{lstlisting}

\subsection{Token Structure}

Struktur token dalam C++ dapat didefinisikan sebagai berikut:

\begin{lstlisting}[language=C++, caption=Struktur Token]
struct Token {
    TokenType type;
    std::string lexeme;
    int line;
    int column;
    union {
        int intValue;      // Untuk INTEGER_LITERAL
        double floatValue; // Untuk FLOAT_LITERAL
    };
    
    Token(TokenType t, const std::string& lex, int l, int c)
        : type(t), lexeme(lex), line(l), column(c) {}
};
\end{lstlisting}

\section{Finite State Machine untuk Lexer}

Lexical analysis secara fundamental adalah proses pattern matching yang dapat dimodelkan menggunakan \textbf{Finite State Machine (FSM)} atau \textbf{Finite Automata}. Menurut sumber dari Aoyama Gakuin University:

\begin{quote}
``Lexical analysis breaks input text into lexemes which correspond to tokens. Usually implemented using regular languages → regex → NFA → DFA → (minimized) DFA for efficiency.''\cite{aoyama2024lexical}
\end{quote}

Dalam implementasi hand-written, kita tidak perlu membuat DFA secara eksplisit, tetapi kita menggunakan logika state machine dalam kode.

\subsection{State Machine Design}

State machine untuk lexer sederhana dapat memiliki state-state berikut:

\begin{itemize}
    \item \textbf{START}: State awal, menunggu karakter pertama dari token
    \item \textbf{IN\_IDENTIFIER}: Sedang membaca identifier atau keyword
    \item \textbf{IN\_NUMBER}: Sedang membaca angka (integer atau float)
    \item \textbf{IN\_FLOAT}: Setelah menemukan titik desimal
    \item \textbf{IN\_STRING}: Sedang membaca string literal
    \item \textbf{IN\_CHAR}: Sedang membaca character literal
    \item \textbf{IN\_COMMENT\_LINE}: Sedang membaca single-line comment
    \item \textbf{IN\_COMMENT\_BLOCK}: Sedang membaca multi-line comment
    \item \textbf{IN\_OPERATOR}: Sedang membaca operator (mungkin multi-character)
    \item \textbf{DONE}: Token selesai dibaca
\end{itemize}

\subsection{State Transitions}

Transisi state terjadi berdasarkan karakter yang dibaca:

\begin{enumerate}
    \item \textbf{START} → \textbf{IN\_IDENTIFIER}: Jika karakter adalah huruf atau underscore
    \item \textbf{START} → \textbf{IN\_NUMBER}: Jika karakter adalah digit
    \item \textbf{START} → \textbf{IN\_STRING}: Jika karakter adalah double quote (\texttt{"})
    \item \textbf{START} → \textbf{IN\_CHAR}: Jika karakter adalah single quote (\texttt{'})
    \item \textbf{START} → \textbf{IN\_COMMENT\_LINE}: Jika menemukan \texttt{//}
    \item \textbf{START} → \textbf{IN\_COMMENT\_BLOCK}: Jika menemukan \texttt{/*}
    \item \textbf{START} → \textbf{IN\_OPERATOR}: Jika karakter adalah operator
    \item \textbf{IN\_NUMBER} → \textbf{IN\_FLOAT}: Jika menemukan titik desimal
    \item \textbf{IN\_IDENTIFIER} → \textbf{DONE}: Jika karakter bukan alphanumeric atau underscore
    \item \textbf{IN\_NUMBER} → \textbf{DONE}: Jika karakter bukan digit atau titik
    \item \textbf{IN\_STRING} → \textbf{DONE}: Jika menemukan closing quote (dengan handling escape)
\end{enumerate}

\section{Implementasi Lexer dalam C++}

Berikut adalah implementasi lengkap lexer sederhana untuk subset bahasa C:

\subsection{Kelas Lexer}

\begin{lstlisting}[language=C++, caption=Header File: lexer.h]
#ifndef LEXER_H
#define LEXER_H

#include <string>
#include <unordered_set>
#include <vector>

enum class TokenType {
    IDENTIFIER,
    KEYWORD_INT, KEYWORD_FLOAT, KEYWORD_IF, KEYWORD_ELSE,
    KEYWORD_WHILE, KEYWORD_FOR, KEYWORD_RETURN,
    INTEGER_LITERAL, FLOAT_LITERAL,
    STRING_LITERAL, CHAR_LITERAL,
    OP_PLUS, OP_MINUS, OP_MULTIPLY, OP_DIVIDE,
    OP_ASSIGN, OP_EQUAL, OP_NOT_EQUAL,
    OP_LESS, OP_LESS_EQUAL, OP_GREATER, OP_GREATER_EQUAL,
    OP_AND, OP_OR, OP_NOT,
    SEMICOLON, COMMA, DOT,
    LPAREN, RPAREN, LBRACE, RBRACE,
    LBRACKET, RBRACKET,
    END_OF_FILE, INVALID
};

struct Token {
    TokenType type;
    std::string lexeme;
    int line;
    int column;
    
    Token(TokenType t, const std::string& lex, int l, int c)
        : type(t), lexeme(lex), line(l), column(c) {}
};

class Lexer {
private:
    std::string input;
    size_t position;
    int line;
    int column;
    std::unordered_set<std::string> keywords;
    
    char peek() const;
    char get();
    void skipWhitespace();
    void skipLineComment();
    void skipBlockComment();
    Token scanIdentifier();
    Token scanNumber();
    Token scanString();
    Token scanChar();
    Token scanOperator();
    TokenType getKeywordType(const std::string& lexeme) const;
    
public:
    Lexer(const std::string& source);
    Token nextToken();
    std::vector<Token> tokenize();
};

#endif
\end{lstlisting}

\subsection{Implementasi Lexer}

\begin{lstlisting}[language=C++, caption=Implementasi: lexer.cpp (Bagian 1)]
#include "lexer.h"
#include <cctype>
#include <stdexcept>

Lexer::Lexer(const std::string& source) 
    : input(source), position(0), line(1), column(1) {
    // Initialize keywords
    keywords = {"int", "float", "if", "else", 
                "while", "for", "return"};
}

char Lexer::peek() const {
    if (position >= input.length()) {
        return '\0';
    }
    return input[position];
}

char Lexer::get() {
    if (position >= input.length()) {
        return '\0';
    }
    char c = input[position++];
    if (c == '\n') {
        line++;
        column = 1;
    } else {
        column++;
    }
    return c;
}
\end{lstlisting}

\subsection{Handling Whitespace dan Komentar}

\begin{lstlisting}[language=C++, caption=Implementasi: lexer.cpp (Bagian 2 - Whitespace dan Comments)]
void Lexer::skipWhitespace() {
    while (position < input.length()) {
        char c = peek();
        if (std::isspace(c)) {
            get();
        } else if (c == '/' && position + 1 < input.length() 
                   && input[position + 1] == '/') {
            skipLineComment();
        } else if (c == '/' && position + 1 < input.length() 
                   && input[position + 1] == '*') {
            skipBlockComment();
        } else {
            break;
        }
    }
}

void Lexer::skipLineComment() {
    // Skip "//"
    get(); get();
    // Skip until newline or EOF
    while (peek() != '\n' && peek() != '\0') {
        get();
    }
}

void Lexer::skipBlockComment() {
    // Skip "/*"
    get(); get();
    while (position < input.length()) {
        if (peek() == '*' && position + 1 < input.length() 
            && input[position + 1] == '/') {
            get(); get(); // Skip "*/"
            return;
        }
        get();
    }
    // Error: unclosed comment
    throw std::runtime_error("Unclosed block comment at line " 
                            + std::to_string(line));
}
\end{lstlisting}

\subsection{Scanning Identifier dan Keyword}

\begin{lstlisting}[language=C++, caption=Implementasi: lexer.cpp (Bagian 3 - Identifier)]
Token Lexer::scanIdentifier() {
    int startLine = line;
    int startCol = column;
    std::string lexeme;
    
    // First character must be letter or underscore
    if (std::isalpha(peek()) || peek() == '_') {
        lexeme += get();
    }
    
    // Subsequent characters can be alphanumeric or underscore
    while (std::isalnum(peek()) || peek() == '_') {
        lexeme += get();
    }
    
    // Check if it's a keyword
    TokenType type = getKeywordType(lexeme);
    if (type != TokenType::IDENTIFIER) {
        return Token(type, lexeme, startLine, startCol);
    }
    
    return Token(TokenType::IDENTIFIER, lexeme, startLine, startCol);
}

TokenType Lexer::getKeywordType(const std::string& lexeme) const {
    if (lexeme == "int") return TokenType::KEYWORD_INT;
    if (lexeme == "float") return TokenType::KEYWORD_FLOAT;
    if (lexeme == "if") return TokenType::KEYWORD_IF;
    if (lexeme == "else") return TokenType::KEYWORD_ELSE;
    if (lexeme == "while") return TokenType::KEYWORD_WHILE;
    if (lexeme == "for") return TokenType::KEYWORD_FOR;
    if (lexeme == "return") return TokenType::KEYWORD_RETURN;
    return TokenType::IDENTIFIER;
}
\end{lstlisting}

\subsection{Scanning Number Literals}

\begin{lstlisting}[language=C++, caption=Implementasi: lexer.cpp (Bagian 4 - Numbers)]
Token Lexer::scanNumber() {
    int startLine = line;
    int startCol = column;
    std::string lexeme;
    bool isFloat = false;
    
    // Read integer part
    while (std::isdigit(peek())) {
        lexeme += get();
    }
    
    // Check for decimal point
    if (peek() == '.') {
        lexeme += get();
        isFloat = true;
        
        // Read fractional part
        while (std::isdigit(peek())) {
            lexeme += get();
        }
    }
    
    // Check for exponent (optional, for future enhancement)
    if (peek() == 'e' || peek() == 'E') {
        lexeme += get();
        if (peek() == '+' || peek() == '-') {
            lexeme += get();
        }
        while (std::isdigit(peek())) {
            lexeme += get();
        }
        isFloat = true;
    }
    
    TokenType type = isFloat ? TokenType::FLOAT_LITERAL 
                              : TokenType::INTEGER_LITERAL;
    return Token(type, lexeme, startLine, startCol);
}
\end{lstlisting}

\subsection{Scanning String dan Character Literals}

\begin{lstlisting}[language=C++, caption=Implementasi: lexer.cpp (Bagian 5 - Strings)]
Token Lexer::scanString() {
    int startLine = line;
    int startCol = column;
    std::string lexeme;
    
    // Consume opening quote
    get(); // Skip opening "
    
    while (peek() != '"' && peek() != '\0') {
        if (peek() == '\\') {
            // Handle escape sequences
            get(); // Skip backslash
            char escaped = get();
            switch (escaped) {
                case 'n': lexeme += '\n'; break;
                case 't': lexeme += '\t'; break;
                case 'r': lexeme += '\r'; break;
                case '\\': lexeme += '\\'; break;
                case '"': lexeme += '"'; break;
                default: lexeme += '\\'; lexeme += escaped; break;
            }
        } else {
            lexeme += get();
        }
    }
    
    if (peek() == '\0') {
        // Unclosed string
        return Token(TokenType::INVALID, lexeme, startLine, startCol);
    }
    
    get(); // Consume closing "
    return Token(TokenType::STRING_LITERAL, lexeme, startLine, startCol);
}

Token Lexer::scanChar() {
    int startLine = line;
    int startCol = column;
    std::string lexeme;
    
    get(); // Skip opening '
    
    if (peek() == '\\') {
        // Escape sequence
        get(); // Skip backslash
        lexeme += get();
    } else {
        lexeme += get();
    }
    
    if (peek() != '\'') {
        return Token(TokenType::INVALID, lexeme, startLine, startCol);
    }
    
    get(); // Consume closing '
    return Token(TokenType::CHAR_LITERAL, lexeme, startLine, startCol);
}
\end{lstlisting}

\subsection{Scanning Operators}

\begin{lstlisting}[language=C++, caption=Implementasi: lexer.cpp (Bagian 6 - Operators)]
Token Lexer::scanOperator() {
    int startLine = line;
    int startCol = column;
    char first = get();
    std::string lexeme(1, first);
    
    // Check for multi-character operators
    char next = peek();
    
    switch (first) {
        case '=':
            if (next == '=') {
                lexeme += get();
                return Token(TokenType::OP_EQUAL, lexeme, startLine, startCol);
            }
            return Token(TokenType::OP_ASSIGN, lexeme, startLine, startCol);
            
        case '!':
            if (next == '=') {
                lexeme += get();
                return Token(TokenType::OP_NOT_EQUAL, lexeme, startLine, startCol);
            }
            return Token(TokenType::OP_NOT, lexeme, startLine, startCol);
            
        case '<':
            if (next == '=') {
                lexeme += get();
                return Token(TokenType::OP_LESS_EQUAL, lexeme, startLine, startCol);
            }
            return Token(TokenType::OP_LESS, lexeme, startLine, startCol);
            
        case '>':
            if (next == '=') {
                lexeme += get();
                return Token(TokenType::OP_GREATER_EQUAL, lexeme, startLine, startCol);
            }
            return Token(TokenType::OP_GREATER, lexeme, startLine, startCol);
            
        case '&':
            if (next == '&') {
                lexeme += get();
                return Token(TokenType::OP_AND, lexeme, startLine, startCol);
            }
            return Token(TokenType::INVALID, lexeme, startLine, startCol);
            
        case '|':
            if (next == '|') {
                lexeme += get();
                return Token(TokenType::OP_OR, lexeme, startLine, startCol);
            }
            return Token(TokenType::INVALID, lexeme, startLine, startCol);
            
        case '+':
            return Token(TokenType::OP_PLUS, lexeme, startLine, startCol);
        case '-':
            return Token(TokenType::OP_MINUS, lexeme, startLine, startCol);
        case '*':
            return Token(TokenType::OP_MULTIPLY, lexeme, startLine, startCol);
        case '/':
            return Token(TokenType::OP_DIVIDE, lexeme, startLine, startCol);
            
        default:
            return Token(TokenType::INVALID, lexeme, startLine, startCol);
    }
}
\end{lstlisting}

\subsection{Main Tokenization Function}

\begin{lstlisting}[language=C++, caption=Implementasi: lexer.cpp (Bagian 7 - Main Function)]
Token Lexer::nextToken() {
    skipWhitespace();
    
    if (position >= input.length()) {
        return Token(TokenType::END_OF_FILE, "", line, column);
    }
    
    char c = peek();
    
    // Identifier or keyword
    if (std::isalpha(c) || c == '_') {
        return scanIdentifier();
    }
    
    // Number
    if (std::isdigit(c)) {
        return scanNumber();
    }
    
    // String literal
    if (c == '"') {
        return scanString();
    }
    
    // Character literal
    if (c == '\'') {
        return scanChar();
    }
    
    // Operators and punctuation
    if (c == '+' || c == '-' || c == '*' || c == '/' ||
        c == '=' || c == '!' || c == '<' || c == '>' ||
        c == '&' || c == '|') {
        return scanOperator();
    }
    
    // Punctuation
    if (c == ';') {
        get();
        return Token(TokenType::SEMICOLON, ";", line, column - 1);
    }
    if (c == ',') {
        get();
        return Token(TokenType::COMMA, ",", line, column - 1);
    }
    if (c == '.') {
        get();
        return Token(TokenType::DOT, ".", line, column - 1);
    }
    if (c == '(') {
        get();
        return Token(TokenType::LPAREN, "(", line, column - 1);
    }
    if (c == ')') {
        get();
        return Token(TokenType::RPAREN, ")", line, column - 1);
    }
    if (c == '{') {
        get();
        return Token(TokenType::LBRACE, "{", line, column - 1);
    }
    if (c == '}') {
        get();
        return Token(TokenType::RBRACE, "}", line, column - 1);
    }
    if (c == '[') {
        get();
        return Token(TokenType::LBRACKET, "[", line, column - 1);
    }
    if (c == ']') {
        get();
        return Token(TokenType::RBRACKET, "]", line, column - 1);
    }
    
    // Unknown character
    get();
    return Token(TokenType::INVALID, std::string(1, c), line, column - 1);
}

std::vector<Token> Lexer::tokenize() {
    std::vector<Token> tokens;
    Token token = nextToken();
    while (token.type != TokenType::END_OF_FILE) {
        tokens.push_back(token);
        token = nextToken();
    }
    tokens.push_back(token); // Add EOF token
    return tokens;
}
\end{lstlisting}

\section{Error Handling}

Error handling dalam lexer harus menangani berbagai kasus edge case:

\subsection{Unclosed Strings dan Comments}

\begin{itemize}
    \item \textbf{Unclosed String}: Jika string literal tidak ditutup sebelum EOF, lexer harus mengembalikan token INVALID dengan informasi posisi yang tepat.
    \item \textbf{Unclosed Block Comment}: Jika komentar blok tidak ditutup, dapat di-handle dengan exception atau mengembalikan error token.
\end{itemize}

\subsection{Invalid Characters}

Karakter yang tidak valid (tidak termasuk dalam kategori token manapun) harus dikembalikan sebagai token INVALID dengan informasi posisi untuk error reporting yang baik.

\subsection{Malformed Numbers}

Contoh kasus malformed:
\begin{itemize}
    \item \texttt{123.} (titik tanpa digit setelahnya)
    \item \texttt{.456} (titik tanpa digit sebelumnya) - dapat di-handle sebagai valid float
    \item \texttt{12.34.56} (multiple decimal points)
\end{itemize}

Implementasi dapat memilih untuk menerima atau menolak format tertentu sesuai kebutuhan.

\section{Testing Lexer}

Unit testing sangat penting untuk memastikan lexer bekerja dengan benar. Berikut contoh test cases:

\subsection{Test Cases untuk Identifier dan Keyword}

\begin{lstlisting}[language=C++, caption=Test Cases: Identifiers dan Keywords]
void testIdentifiers() {
    Lexer lexer("int x = 42;");
    Token t1 = lexer.nextToken(); // Should be KEYWORD_INT
    Token t2 = lexer.nextToken(); // Should be IDENTIFIER "x"
    Token t3 = lexer.nextToken(); // Should be OP_ASSIGN
    // ...
}
\end{lstlisting}

\subsection{Test Cases untuk Numbers}

\begin{itemize}
    \item \texttt{42} → INTEGER\_LITERAL
    \item \texttt{3.14} → FLOAT\_LITERAL
    \item \texttt{123.456} → FLOAT\_LITERAL
    \item \texttt{0} → INTEGER\_LITERAL
\end{itemize}

\subsection{Test Cases untuk Strings}

\begin{itemize}
    \item \texttt{"hello"} → STRING\_LITERAL dengan value "hello"
    \item \texttt{"hello\textbackslash{}nworld"} → STRING\_LITERAL dengan escape sequence
    \item \texttt{"unclosed} → INVALID (unclosed string)
\end{itemize}

\subsection{Test Cases untuk Comments}

\begin{itemize}
    \item \texttt{// single line comment} $\rightarrow$ Di-skip, tidak menghasilkan token
    \item \texttt{/* multi-line comment */} $\rightarrow$ Di-skip
    \item \texttt{/* unclosed comment} → Error atau exception
\end{itemize}

\section{Contoh Penggunaan}

Berikut contoh penggunaan lexer untuk tokenize source code sederhana:

\begin{lstlisting}[language=C++, caption=Contoh Penggunaan Lexer]
#include "lexer.h"
#include <iostream>

int main() {
    std::string source = R"(
        int x = 42;
        float y = 3.14;
        if (x > 10) {
            return y;
        }
    )";
    
    Lexer lexer(source);
    std::vector<Token> tokens = lexer.tokenize();
    
    for (const auto& token : tokens) {
        std::cout << "Token: " << token.lexeme 
                  << " Type: " << static_cast<int>(token.type)
                  << " Line: " << token.line 
                  << " Column: " << token.column << std::endl;
    }
    
    return 0;
}
\end{lstlisting}

Output yang diharapkan:
\begin{verbatim}
Token: int Type: 1 Line: 2 Column: 9
Token: x Type: 0 Line: 2 Column: 13
Token: = Type: 13 Line: 2 Column: 15
Token: 42 Type: 8 Line: 2 Column: 17
Token: ; Type: 20 Line: 2 Column: 19
...
\end{verbatim}

\section{Best Practices}

Beberapa best practices dalam implementasi hand-written lexer:

\begin{enumerate}
    \item \textbf{Separation of Concerns}: Pisahkan logika untuk setiap jenis token ke fungsi terpisah
    \item \textbf{Position Tracking}: Selalu track line dan column untuk error reporting yang baik
    \item \textbf{Lookahead}: Gunakan \texttt{peek()} untuk lookahead tanpa mengkonsumsi karakter
    \item \textbf{Error Recovery}: Rancang strategi error recovery (misalnya skip invalid character dan lanjut)
    \item \textbf{Testing}: Buat comprehensive test suite untuk semua edge cases
    \item \textbf{Documentation}: Dokumentasikan token types dan format yang didukung
\end{enumerate}

\section{Kesimpulan}

Dalam bab ini, kita telah mempelajari:

\begin{enumerate}
    \item Struktur token dan token types untuk subset bahasa C
    \item Konsep finite state machine dalam konteks lexical analysis
    \item Implementasi hand-written lexer dalam C++ dengan handling:
    \begin{itemize}
        \item Identifier dan keyword recognition
        \item Number literals (integer dan float)
        \item String dan character literals dengan escape sequences
        \item Operators (single dan multi-character)
        \item Whitespace dan komentar (single-line dan multi-line)
    \end{itemize}
    \item Error handling untuk edge cases
    \item Testing strategies untuk lexer
\end{enumerate}

Implementasi hand-written lexer memberikan pemahaman mendalam tentang proses tokenization dan menjadi dasar untuk memahami bagaimana lexer generator seperti Flex bekerja di belakang layar.

\section{Latihan}

\begin{enumerate}
    \item \textbf{Implementasi Dasar}: Implementasikan lexer sederhana yang dapat mengenali:
    \begin{itemize}
        \item Identifier (huruf, angka, underscore)
        \item Integer literals
        \item Operator dasar: \texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}, \texttt{=}
        \item Punctuation: \texttt{;}, \texttt{,}, \texttt{(}, \texttt{)}
    \end{itemize}
    
    \item \textbf{Handling Comments}: Tambahkan support untuk:
    \begin{itemize}
        \item Single-line comments (\texttt{//})
        \item Multi-line comments (\texttt{/* */})
        \item Error handling untuk unclosed block comments
    \end{itemize}
    
    \item \textbf{String Literals}: Implementasikan scanning untuk string literals dengan:
    \begin{itemize}
        \item Support escape sequences: \texttt{\textbackslash n}, \texttt{\textbackslash t}, \texttt{\textbackslash "}, \texttt{\textbackslash\textbackslash}
        \item Error handling untuk unclosed strings
    \end{itemize}
    
    \item \textbf{Float Literals}: Extend number scanning untuk mendukung:
    \begin{itemize}
        \item Float dengan decimal point: \texttt{3.14}
        \item Scientific notation: \texttt{1.5e10}, \texttt{2.3E-5}
    \end{itemize}
    
    \item \textbf{Unit Testing}: Buat test suite yang mencakup:
    \begin{itemize}
        \item Valid tokens dari semua kategori
        \item Edge cases (unclosed strings, invalid characters, dll.)
        \item Position tracking (line dan column)
    \end{itemize}
    
    \item \textbf{Error Recovery}: Implementasikan error recovery strategy:
    \begin{itemize}
        \item Skip invalid characters dan lanjut scanning
        \item Report multiple errors dalam satu pass jika memungkinkan
    \end{itemize}
    
    \item \textbf{Performance}: Analisis dan optimasi:
    \begin{itemize}
        \item Bandingkan performa dengan lexer generator (jika tersedia)
        \item Identifikasi bottleneck dalam implementasi
    \end{itemize}
\end{enumerate}

\section{Referensi dan Bahan Bacaan Lanjutan}

Untuk memperdalam pemahaman tentang implementasi lexer, mahasiswa disarankan membaca:

\begin{itemize}
    \item \textbf{Dragon Book}: Aho, Lam, Sethi, \& Ullman (2006). \textit{Compilers: Principles, Techniques, and Tools} \cite{aho2006compilers} - Bab 3: Lexical Analysis
    
    \item \textbf{Engineering a Compiler}: Cooper \& Torczon (2011) \cite{cooper2011engineering} - Bab 2: Scanning
    
    \item \textbf{OpenGenus - Build Lexer}: Tutorial tentang hand-written lexer \cite{opengenus2024lexer}
    
    \item \textbf{Aoyama Gakuin University}: Lecture notes tentang lexical analysis \cite{aoyama2024lexical}
    
    \item \textbf{GeeksforGeeks}: Contoh implementasi lexical analyzer dalam C++ \footnote{\url{https://www.geeksforgeeks.org/cpp/lexical-analyzer-in-cpp/}}
    
    \item \textbf{Programming Notes}: Tutorial tentang simple lexer menggunakan finite state machine \footnote{\url{https://www.programmingnotes.org/4699/cpp-simple-lexer-using-a-finite-state-machine/}}
\end{itemize}
