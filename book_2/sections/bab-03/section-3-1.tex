\section{Pengenalan Lexical Analysis}

\subsection{Peran Lexical Analyzer}

\compiler{Lexical Analysis} (atau \compiler{Lexer/Scanner}) adalah garda terdepan dalam proses kompilasi. Tugas utamanya adalah membaca aliran karakter mentah dari kode sumber dan mengonversinya menjadi aliran \textit{token} yang terstruktur. Bayangkan lexer sebagai seorang asisten yang membaca teks panjang, membuang spasi yang tidak perlu, dan mengelompokkan huruf-huruf menjadi kata-kata yang bermakna (seperti "run", "jump", "fast") sebelum diserahkan kepada ahli tata bahasa (Parser).

Mengapa memisahkan Lexer dari Parser? Alasan utamanya adalah efisiensi dan modularitas:
\begin{itemize}
  \item \textbf{Efisiensi I/O}: Lexer menangani tugas berat membaca file karakter demi karakter. Dengan memisahkannya, kita bisa menerapkan teknik \textit{buffering} yang agresif di level ini tanpa membebani parser.
  \item \textbf{Penyederhanaan Parser}: Parser tidak perlu khawatir tentang spasi atau komentar; ia hanya melihat aliran token bersih. Ini membuat tata bahasa (\textit{grammar}) parser menjadi jauh lebih sederhana.
\end{itemize}

\subsection{Alur Teoretis Lexical Analysis}

Pembangunan lexer tidak dilakukan secara sembarangan, melainkan didasarkan pada fondasi matematika yang kokoh yaitu teori \textit{Automata}. Alur transformasi dari spesifikasi pola hingga menjadi kode program yang berjalan adalah sebagai berikut:

\begin{figure}[!htbp]
    \centering
    \adjustbox{max width=0.85\textwidth,center}{%
    \begin{tikzpicture}[
        box/.style={rectangle, draw=blue!50, fill=blue!10, text width=1.8cm, text centered, minimum height=0.8cm, rounded corners, font=\footnotesize, inner sep=4pt, align=center},
        arrow/.style={->, >=stealth, thick},
        label/.style={font=\tiny, above, align=center},
        node distance=1.8cm
    ]
    \node[box] (regex) {Regular\\Expression};
    \node[box, right=of regex] (nfa) {$\epsilon$-NFA};
    \node[box, right=of nfa] (dfa) {DFA};
    \node[box, right=of dfa] (impl) {Scanner};
    
    \draw[arrow] (regex) -- node[label] {Thompson} (nfa);
    \draw[arrow] (nfa) -- node[label] {Subset\\Const.} (dfa);
    \draw[arrow] (dfa) -- node[label] {Code\\Gen.} (impl);
    \end{tikzpicture}%
    }
    \caption{Alur konversi dari regular expression ke implementasi scanner}
    \label{fig:lexical-theory-overview}
\end{figure}

Kita mendeskripsikan pola token menggunakan \textit{Regular Expression}, mengubahnya menjadi mesin abstrak non-deterministik (NFA), mengonversinya menjadi mesin deterministik (DFA) yang efisien, dan akhirnya menerjemahkannya menjadi kode (C/C++) \cite{aoyama2024lexical}.

\subsection{Token, Lexeme, dan Pattern}

Penting untuk membedakan tiga istilah ini:
\begin{itemize}
  \item \textbf{Token}: Simbol abstrak yang mewakili jenis unit leksikal (misal: \keyword{Token::ID}, \keyword{Token::NUMBER}). Ini adalah output yang dilihat oleh parser.
  \item \textbf{Lexeme}: Deretan karakter aktual dalam kode sumber yang cocok dengan suatu pola token (misal: variabel \code{count}, angka \code{3.14}).
  \item \textbf{Attribute}: Karena banyak lexeme (seperti \code{10} dan \code{20}) bisa memiliki token yang sama (\keyword{INTEGER}), lexer perlu menyimpan informasi tambahan (nilai lexeme itu sendiri) agar fase semantik nanti tahu angka mana yang dimaksud.
\end{itemize}
